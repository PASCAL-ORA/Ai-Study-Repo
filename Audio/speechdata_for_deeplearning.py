# -*- coding: utf-8 -*-
"""speechData_for_deepLearning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1afArg4MNA0QHUvQYrXGfKz6yYb8c8Lmq

<h1>Audio Data</h1>

- 음파(sound wave): 연속적인 아날로그 신호
- **디지털 오디오 포맷**: WAV, FLAC, MP3(압축 방식 차이)
- 디지털화 과정:
  1. 마이크 -> 음파를 전기 신호로 변환
  2. **ADC(Analog-to-Digital Converter)** -> 샘플링 통해 디지털 표현

<h1>Sampling</h1>

- 샘플링: 연속 신호를 일정 간격으로 측정하는 과정
- 샘플링 속도(Sampling Rate)
  - 단위: Hz(1초 동안 채취되는 샘플 수)
  - 예시
    - CD 품질: 44.1Hz
    - 고해상도 오디오: 192kHz
    - **음성 모델**: 보통 16kHz

- 나이퀴스트 한계: 최대 표현 가능한 주파수 = 샘플링 속도의 1/2

- 모델 일관성 유지를 위해 학습용 데이터는 모두 동일한 샘플링 속도 유지

- upsampling: 시간이 거의 소요되지 않음

- 오디오 작업을 할 때는 데이터셋에 있는 모든 오디오 예제가 동일한 샘플링 속도를 가지고 있는지 확인해야 함 (모델 일관성 유지)

- 커스텀 데이터로 사전 학습된 모델을 파인튜닝(이미 사전 학습된 모델을 특정 작업이나 데이터에 맞게 추가로 학습)할 계획이면 사전 학습된 데이터의 샘플링 속도와 커스텀 데이터셋의 샘플링 속도가 일치해야 함
  1. 다른 샘플링 속도의 데이터를 입력하면 시간축 정보가 왜곡되어 모델이 올바르게 처리할 수 없음
  2. 주파수 도메인 특성 변화: 샘플링 속도가 다르면 나이퀴스트 주파수가 달라져 주파수 스펙트럼의 범위가 변화
  3. 입력 차원 불일치: 입력 텐서의 차원 불일치
  4. 학습된 특징의 스케일 불일치

- 오디오 작업을 처리하는 트랜스포머 모델은 시퀀스를 취급하며 어텐션 매커니즘을 이용해 오디오 또는 멀티모달 표현 학습

- 서로 다른 샘플링 속도를 갖는 오디오 데이터는 다른 시퀀스가 되므로 모델이 샘플링 속도간 일반화를 하기 어려움

- 리샘플링은 이런 서로 다른 샘플링 속도를 일치시켜주는 작업, 전처리 과정 중 하나

<h1>진폭(Amplitude)과 비트뎁스(Bit Depth)</h1>

- 진폭: 소리의 세기(입력 수준), 음파의 최대 변위(우리가 느끼는 소리의 크기와 직결) -> 단위: dB
  - dB(데시벨) 스케일의 특징:
    - 로그 스케일: 인간의 청각 특성에 맞춰 설계됨
    - 상대적 측정: 기준값 대비 얼마나 큰지를 표현
    - 실용적 범위: 0dB(들을 수 있는 최소 소리) ~ 120dB(고통 역치)
    - 6dB 증가 = 2배 증폭: 예를 들어 60dB → 66dB는 실제 에너지가 2배
- 비트뎁스: 진폭을 얼마나 정밀하게 표현할 것인지
  - 16bit: 약 65,536단계
  - 24bit: 약 1,600만 단계
- 양자화 노이즈: 이산화 과정에서 생기는 오차 -> 높은 비트뎁스일수록 감소
  - 양자화 노이즈의 매커니즘
    - 발생 원리: 연속적인 아날로그 신호를 이산적인 디지털 값으로 변환할 때, 실제값과 가장 가까운 디지털 값 사이의 차이가 노이즈가 됨
- 부동소수점 포맷: 범위 [-1.0, 1.0], 머신러닝 모델의 입력 형태로 적합

<h1>Speech Data</h1>
1. 음성 파형(Wave Form): 음성 파일<br>
2. 스펙트로그램(Spetrogram)<br>
3. Utterance(사용자의 말 / Text): 발화 텍스트<br>
4. (Optional) Alignment 정렬: 어디서부터 어디까지가 텍스트 어느 부분까지 해당되는지<br><br>

음성 데이터는 Waveform 파일로 저장됨
- 파형: 시간에 따른 직폭 변화를 보여주는 그래프
- 시간에 따른 샘플 값들을 그래프로 표현하여 소리의 진폭 변화를 보여줌. 이를 소리의 시간 영역(time domain)표현이라 함
- Waveform: 세기 표현. 16000Hz로 녹음된 음성이라고 하면 1/16000초 마다 들리는 소리가 어느 정도의 세기를 가지고 있는지 기록한 것<br><br>

Waveform 형태의 데이터는 전처리를 통해서 유의미한 정보를 가지고 있는 어떤 형태로 만든다

Waveform은 푸리에 변환(Fourier transform, FT)을 거쳐서 Spectogram이라는 피쳐로 바꿀 수 있음

이렇듯 음성 파형을 변환하는 이유는 음성에 들어있는 정보를 음성 신호 / 파형에서 바로 얻어낼 수 없고 수학적인 신호 처리를 거쳐서 추출할 수 있기 때문이다
- 푸리에 변환 함수: 특정 시간 길이의 음성 조각(프레임)이 각각의 주파수 성분들을 얼마만큼 갖고 있는지를 의미하는 스펙트럼을 얻을 수 있다

음성 전체로부터 얻은 여러 개의 스펙트럼을 시간 축에 나열하면 시간 변화에 따른 프텍트럼의 변화인 스펙트로그램을 얻게 됨
"""

import librosa
import matplotlib.pyplot as plt
import librosa.display

array, sampling_rate = librosa.load(librosa.ex("trumpet"))
plt.figure().set_figwidth(12)
librosa.display.waveshow(array, sr=sampling_rate)

"""- x축: 시간
- y축: 진폭

각 점은 이소리를 샘플링할때 취한 값 librosa가 이미 오디오를 부동소수점으로 변환해서 진폭값이 [-1.0,1.0] 범위에 존재

***plt.figure().set_figwidth(12):***
- 새로운 figure 생성하고 가로 크기를 12인치로 설정
- 파형을 보기 좋게 넓게 표시

***librosa.display.waveshow():***
- 시간축(x) vs 진폭(y) 그래프로 파형 표시
- **sr 매개변수**: 올바른 시간축 스케일을 위해 샘플링 레이트 전달

<h1>주파수 스펙트럼</h1>

- 이산 푸리에 변환(DFT) 사용 -> 오디오를 구성하는 주파수와 진폭 확인
  - 푸리에 변환: 복잡한 신호를 여러 주파수의 사인파 합으로 분해하는 도구 -> DFT는 연속 푸리에 변환을 디지털 세계에서 실제로 구현할 수 있도록 한 것
- x축: 주파수(Hz, 로그 스케일)
- y축: 진폭(dB)
- np.fft.rfft()와 librosa.amplitude_to_db() 등을 활용
- 시간 정보 포함 X
"""

import numpy as np

dft_input = array  # 이전에 로드한 오디오 신호 배열을 DFT 입력으로 설정

# calculate the DFT (DFT 계산)
window = np.hanning(len(dft_input))  # 한닝 윈도우 생성 - 스펙트럼 누설을 방지하고 부드러운 가장자리 처리
windowed_input = dft_input * window  # 원본 신호에 윈도우를 곱해서 끝부분을 부드럽게 처리
dft = np.fft.rfft(windowed_input)    # 실수 FFT 수행 - 양의 주파수만 계산 (대칭성 이용)

# get the amplitude spectrum in decibels (진폭 스펙트럼을 데시벨로 변환)
amplitude = np.abs(dft)                                    # 복소수 DFT 결과의 크기(진폭) 계산
amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)  # 진폭을 dB 스케일로 변환, 최댓값을 0dB 기준으로 설정

# get the frequency bins (주파수 빈 계산)
frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))  # 각 DFT 빈에 해당하는 실제 주파수 값 계산

# 시각화
plt.figure().set_figwidth(12)  # 그래프 가로 크기를 12인치로 설정
plt.plot(frequency, amplitude_db)  # 주파수(x축) vs 진폭dB(y축) 그래프 그리기
plt.xlabel("Frequency (Hz)")       # x축 라벨: 주파수 (헤르츠)
plt.ylabel("Amplitude (dB)")       # y축 라벨: 진폭 (데시벨)
plt.xscale("log")                  # x축을 로그 스케일로 설정 - 저주파~고주파 영역을 균등하게 보기 위함

"""- 오디오 구간에 존재하는 다양한 주파수 세기를 보여줌
  x축: 로그스케일로 주파수 표시
  y축: 진폭
- 여러 개의 피크를 보여줌: 연주 중인 음표의 고주가에 해당, 더 높은 소주파는 더 작은 소리를 나타냄
- 첫번째 피크가 약 620 Hz에 있으므로 이 스펙트럼은 E♭ 음표의 주파수 스펙트럼
- DFT 결과값: 복소수 배열
- 진폭 정보, np.abs(dft):
  - 각 주파수 성분의 세기/강도
  - 스펙트로그램에서 보는 밝기/색상 강도
  - 얼마나 큰 소리인지
- 위상 스펙트럼(phase spectrum, 실수부와 허수부 사이의 각도):
  - 각 주파수 성분의 시간적 위치/차이밍
  - 사인파가 시작되는 시점
  - 언제 시작하는지
- librosa.amplitude_to_db()는 진폭값을 데시벨 스케일로 변환
  -> 스펙트럼의 더욱 세밀한 부분까지 쉽게 확인 가능, 때때로 파워 스펙트럼 사용
- 진폭보다 에너지를 측정하기 위해 사용하나 단지 진폭에 제곱을 취한 값으로 나타낸 스펙트럼
- 주파수 스펙트럼에서는 시간 정보 X, 주파수에 해당하는 진폭을 확인 가능
- 푸리에 변환을 통해 하나의 소리에서 각기 다른 주파수를 파악해서 진폭 확인 가능

<h1>Traditional Speech to Text -> End-to-End Deep Learning</h1>

전통적인 STT에서는 Feature Extraction(푸리에 변환)을 거치고, Acoustic Model + Lexicon + Language Model을 합쳐 decoder로 만들어 각각의 과정을 거쳐 조합하는 과정이었다면 2010년대 초반 딥러닝이 나와 이 모든 것을 하나로 할 수 있다 = 각각의 과정이 개별적으로 필요 없다는 개념이 등장해 End-to-End(E2E)라고 부르기 시작함

E2E로 훈련되더라도 아직 필요한 것들
- Feature Extraction 과정 - 스펙트로그램(Spectrogram)을 뽑아내는 푸리에 변환 과정
- Beam search decoder
- Language model - (Beam search)에 필요
  - 모델의 학습과 별개로 음성 자료를 문자열로 바꾸는 디코딩에 이용되는 빔 탐색

<h1>스펙트로그램(Spetrogram)</h1>

- 시간 + 주파수 + 진폭 정보를 시간화한 2D 이미지
- STFT(Short-Time Fourier Transform) 사용 -> 짧은 구간별 DFT
- 색상 강도는 각 시점의 주파수 성분의 진폭 또는 파워(dB)를 의미
"""

import numpy as np

D = librosa.stft(array)  # 단시간 푸리에 변환(STFT) 수행 - 시간에 따른 주파수 변화를 분석하기 위해 오디오를 작은 윈도우로 나누어 각각 FFT 계산
S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)  # STFT 결과의 복소수 크기를 구하고 dB 스케일로 변환, 최댓값을 0dB 기준으로 설정

# 스펙트로그램 시각화
plt.figure().set_figwidth(12)  # 그래프 가로 크기를 12인치로 설정
librosa.display.specshow(S_db, x_axis="time", y_axis="hz")  # 스펙트로그램을 2D 히트맵으로 표시 - x축: 시간, y축: 주파수, 색상: 진폭(dB)
plt.colorbar()  # 색상 막대 추가 - dB 값과 색상의 대응 관계를 보여줌

"""- 스펙트럼의 문제는 주어진 한 순간만의 주파수들을 보여주는 것
  - 해결법: 시간을 작은 구간들로 나누어 DFT를 적용하고, 그 결과인 스펙트럼들을 쌓아 스펙트로그램을 만드는 것
- 이를 통해 시간, 주파수, 진폭을 그래프에서 한 눈에 볼 수 있음 -> 이 계산을 수행하는 알고리즘: STFT(Short Time Rourier Transform)
- 스펙트로그램은 일반적으로 오디오 신호와 몇 밀리초 정도 되는 짧은 구간에 DFT를 적용하여 주파수 스펙트럼들을 얻어 만들어짐
- X축은 시간을 나타내며 Y축은 주파수를 Hz 단위로 나타냄
- 색상의 강도는 각 시점의 주파수 성분의 진폭 또는 파워를 데시벨(dB)로 측정하여 나타냄
- 시간에 따른 hz의 범위에 맞는 색깔로 db를 표기

<h1>모델 1. Acoustic Model(Deep Speech 2)</h1>

2015년에 나온 Deep Speech 2 논문에서 공개한 End-to-End 음서 인식 모델

Input으로 Spectrogram을 받고 Output으로 CTC(Connectionist Temporal Classification)을 반환하는 것을 볼 수 있다
- 음성 인식에 유리한 모델 아키텍쳐
- 호율적인 학습 테크닉: CTC Loss 직접 구현 등

입력받은 Spectrogram에서 중요한 특징을 뽑아내는 레이어로 CNN을 사용, 이후 양방향 (bidirectional) RNN을 두고, 마지막에는 Fully Conencted Layer

**CTC: Connectionist Temporal Classification(연결성 시계열 분류기)**

CTC: 음성의 길이에 맞게 텍스트의 길이를 늘린다는 개념
- 음성의 길이(S)가 7, 텍스트(T)가 3이라면 모자란 만큼 Blank("_")로 채운다. 하지만 우리는 어디서 Blank가 들어가고 어디서 실제 Text가 들어가야 되는지 모른다.
- 따라서 모든 조합의 합으로 확률 표시 -> CTC는 최적의 정렬(Alignment)를 찾아내기 위해 가능한 모든 시퀀스들을 나열

CTC의 장점
- Encoder만 쌓고, 그 위에 CTC Loss만 넣으면 음성 인식 가능
- 어느 정도 정렬 위치 추출 가능

단점: 추가적인 LM decoding(Beam Search Decoding)이 없으면 성능이 극적으로 좋아지지 않음

<h1>모델 2. LAS: Listen, Attend, Spell</h1>
Deep Speech 2와 비슷한 시기에 제안한 모델

Attention 알고리즘을 이용하여 음향 입력과 레이블 출력 사이의 정렬(Alignment)을 계산

- Listener: 피라미드 형식으로 구현된 bidirectional(BLSTM) 인코더
  - 입력 시퀀스 x로부터 특징을 뽑아냄
  - BLSTM을 Paramidal 형식으로 3개를 붙여 사용 -> pBLSTM (1개당 연산 속도를 2배로 줄여줌)

- Speller: attention을 사용해서 출력을 하는 디코더
  - AttendAndSpell 함수는 attention 기반의 LSTM 변환기인 decoder를 통해서 계산
  - decoder는 매 tinme step 마다 이전에 결정된 문자들에 대한 다음 문자의 분포 생성

이 모델의 학습은 입력 음성에 대해 알는 sequence의 log probability를 maximize한다

그리고 디코딩 관련하여, 테스트 시에 가장 근접한 character sequence를 주어진 음향에 대해 찾는다

Contenr based Attention Mechanism은 문자와 오디오 신호 사이의 명확한 정렬을 생성
=> ***어떤 텍스트를 만들 때 어디를 봐야 하는지 ***

<h1>모델 3. RNN-T: Recurrent Neural Network Transducer</h1>

CTC와 유사

CTC와 마찬가지로 최적의 정렬(Alignment)을 찾아내기 위해 모든 시퀀스를 나열한 후 구함

하지만 CTC와 달리 조건부 독립을 가정하기 않기 때문에 최적 경로 계산이 CTC와 다르며 CTC보다 더 복잡하게 계산

가능한 모든 경로, path의 조합으로 확률을 계산하고, 그 확률을 높이는 방향으로 훈련

<h1>1. Audio Auto Tagging, Speech Recognition(STT)</h1>
<h2>1.1 Audio Auto Tagging</h2>

- Audio Auto Tagging(음향 이벤트 인식): 오디오 신호에서 발생하는 이벤트 종류를 찾는 문제
  - 유사한 소리가 동시에 포함된 오디오는 해당 소리들을 인식하기 어려워함
  - 따라서 특정 오디오에 대해 해당 오디오가 포함하는 여러 이벤트들을 tagging 할 수 있는 Multi-Label Classification이 가능한 Audio Auto Tagging System 필요<br><br>

<img src = "https://images.velog.io/images/tobigsvoice1516/post/35535327-8698-4a73-a528-e9176da03aea/AudioAutoTagging.png">

<h3>1.1.1 Audio Auto Tagging Metric: label-weighted label-rangking average precision</h3>

보통 Audio Auto Tagging을 평가할 때는 Average Precision, Rank-Average Precision과 같이 IR에서 사용되는 metric이 많이 사용
- 딥러닝 모델들을 활용해 Multi-Label Classification을 진행하면, 각 레이블에 대한 점수가 확률벡터로 나오게 될텐데, 실제 1인 Label에 대해서 얼마나 높은 Rank로 모델이 예측하였는가가 결국 모델이 잘 예측했냐를 평가할 수 있기 때문에 IR 쪽에서 사용되는 metric이 자주 사용

<h3>1.1.2 Audio Auto Tagging Procedure</h3>

Wave Form 형태의 오디오를 Feature Extraction을 통해 모델의 Input으로 들어갈 수 있게 만들어 주고 (Mel Spectogram) 우리의 가정으로 만들어진 모델(CNN, RNN)을 통해 Feature Extraction을 진행 -> 이후 Feature Space에서 Classifier 학습

<img src = "https://images.velog.io/images/tobigsvoice1516/post/ab46aa7d-fb97-469d-8c5f-42b8c0622d40/AATProcedure.png">

<br>

<b>오디오마다 Time Sequence가 다르면?</b>

모델이 동일한 Time Sequence를 처리하도록 Train Data를 Chunk로 나누어 주고 Label을 복사하면 된다

<img src = "https://images.velog.io/images/tobigsvoice1516/post/1c49c6ba-61a4-4eae-aada-ebe3261f9c15/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-10-30%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.45.56.png">
Mel-Power Spectogram

<h1>멜 스펙토그램(Mel Spectrogram)</h1>

- 스펙트로그램의 지각 기반 확장
- Mel 스케일: 인간 청각의 비선형 반응을 근사
- Mel Filterbank로 주파수를 변환해 사람의 인지에 맞는 표현
- 일반적으로 n_mels = 40 ~ 128선
"""

S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)  # 멜 스펙트로그램 계산 - 인간 청각에 맞춘 주파수 스케일, 128개 멜 밴드, 최대 8kHz까지 분석
S_dB = librosa.power_to_db(S, ref=np.max)  # 파워 스펙트럼을 dB 스케일로 변환 - 최댓값을 0dB 기준으로 설정
plt.figure().set_figwidth(12)  # 그래프 가로 크기를 12인치로 설정
librosa.display.specshow(S_dB, x_axis="time", y_axis="mel", sr=sampling_rate, fmax=8000)  # 멜 스펙트로그램을 2D 히트맵으로 표시 - x축: 시간, y축: 멜 스케일, 색상: 파워(dB)
plt.colorbar()  # 색상 막대 추가 - dB 값과 색상의 대응 관계를 보여줌

"""- 멜 스펙트로그램은 스펙트로그램의 한 종류로 음성 작업이나 머신러닝 작업에 주로 사용
- 오디오 신호를 시간에 따른 주파수로 보여준다는 점에서 스펙트로그램과 비슷하지만, 다른 주파수 축 사용
- 표준적인 스페그로그램에서는 주파수 축이 선형이며 Hz 단위로 측정
- 하지만 사람의 청각 시스템은 고주파보다 저주파에 더 민감하며, 이 민감성은 주파수가 증가함에 따라 로그함수적으로 감소
- 멜 스케일은 이런 사람의 비선형 주파수 반응을 근사한 지각 스케일
- 멜 스펙토그램을 만드려면 전처럼 STFT를 사용하고 오디오를 여러 짧은 구간으로 나눠 일련의 주파수 스펙트럼들을 얻어야 함
  - 그 후 각 스펙트럼에 mel filterbank라고 불리는 필터들을 적용시켜 주파수를 멜 스케일로 변환
- 위의 예에서 n_mels는 mel band의 수를 지정, mel band는 필터를 이용해 스펙트럼을 지각적으로 의미있는 요소로 나누는 주파수 범위의 집합을 정의
- 이 필터들의 모양과 간격은 사람의 귀가 다양한 주파수에 반응하는 방식을 모방하도록 선택
  - 흔히 n_mels의 값으로 40 또는 80이 선택, fmax는 우리가 관심을 가지는 최고 주파수를 나타냄
- 일반적인 스펙트로그램과 마찬가지로 멜 스펙트로그램의 주파수 성분 역시 세기를 데시벨로 표현하는 것이 일반적
- 데시벨로의 변환이 로그 연산을 포함하기 때문에 이를 흔이 로그-멜 스펙트로그램이라 함

<h2>1.2 Speech Recognition(STT)</h2>

음성 인식: 사람이 말하는 음성 언어를 컴퓨터가 해석해 그 내용을 문자 데이터로 전환하는 처리 (STT)

<h1>2. CTC Loss</h1>
<h2>2.1 Deep Speech2</h2>
<img src = "https://images.velog.io/images/tobigsvoice1516/post/bdfd0a6c-01b9-4d09-95a4-ac8716347d5a/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-10-30%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2011.02.06.png">

<br><br>

<h2>2.2 CTC(Connectionist Temporal Classification) Loss(Speech Recognition Loss Finction)</h2>
<h3>Why we euse CTC Loss?</h3><br>
일반적인 Speech Recognignition에서 우리는 데이터셋으로 오디오 클립과, Ptranscript를 받게 된다.

하지만 우리는 어떤 단어의 Charater가 Audio와 Alignment가 맞는지 알 수 없음(2초 동안 조용하다가 Hello를 말하거나 바로 Hello를 말하거나 둘다 Transcript은 Hello)

이러한 Alignment 없이, 어떤 Audio와 Text 사이의 규칙을 정의하긴 힘듦. 또한 사람들의 언어 사용 방식이 다양하기 때문에 단일한 Rule로 정의하기 또한 힘듦

<img src = "https://images.velog.io/images/tobigsvoice1516/post/73b7c67a-3de7-439e-b8ad-a08b01ad8915/CTC1.png">

input와 output 사이의 정확한 Alignment가 labeling되어있는 데이터셋이 필요하진 않으나 주어진 input에 대해서 output의 확률값은 필요하다

CTC는 둘 사이의 가능한 모든 alignment의 가능성을 합산하여 작용

(CTC works by summing over the probability of all possible alignments between the two). 이 말은 즉, Input도 output의 가능한 Align을 모두 뽑아서 Marginalize하자! 라는 뜻

<h1>파이썬 라이브러리로 음성 데이터 전처리</h1>
"""

# Library 1. Librosa
import librosa
sr = 22050
data, sample_rate = librosa.load(path, sr=sr)

# Library 2. Scipy-Wavfile
from scipy.io import wavfile
sample_rate, data = wavfile.read(path)

# Library 3. Tensorflow
data, sample_rate = tf.audio.decode_wav(tf.io.read_file(path))

# Library 4. Torchaudio
import torchaudio
data, sample_rate = torchaudio.load(path)

"""- data: 오디오 파일의 파형 데이터를 배열로 반환한 것
- sample_rate: 오디오 데이터의 샘플링 속도, 초당 샘플링된 음성 데이터 포인트의 수, Hz단위
"""

# Audio Preprocessing

def trimming(data, sampling_rate, top_db = 60):

    frame_length = 0.025   # 0.025 ~ 0.05
    frame_stride = 0.010   # frame_length/2

    input_nfft = int(round(sampling_rate*frame_length))
    input_stride = int(round(sampling_rate*frame_stride))

    if len(np.shape(data)) == 1:
        trim_data = librosa.effects.trim(data, top_db=top_db, frame_length=input_nfft, hop_length=input_stride)[0]
    else:
        trim_data = data.apply(lambda x : librosa.effects.trim(x, top_db=top_db, frame_length=input_nfft, hop_length=input_stride)[0])

    return trim_data

"""Trimming
- 60dB 이하를 무음으로 처리
  - top_db: 음성 신호의 최대 소음 레벨을 나타내는 변수로 이 값보다 작은 레벨의 소음은 삭제
  - frame_length:  프레임 길이를 나타내는 변수로, 음성 데이터를 프레임으로 나눌 때 사용
  - frame_stride: 프레임 간격을 나타내는 변수로, 연속적인 프레임 사이의 간격을 조절
"""

def padding(x, reqlen=100000):
    x_len = x.shape[0]
    if reqlen < x_len:
        max_offset = x_len - reqlen
        offset = np.random.randint(max_offset)
        x = x[offset:(reqlen+offset)]
        return x
    elif reqlen == x_len:
        return x
    else:
        total_diff = reqlen - x_len
        offset = np.random.randint(total_diff)
        left_pad = offset
        right_pad = total_diff - offset
        return np.pad(x, (left_pad, right_pad), 'wrap')

"""Random Padding
- 음성 데이터들의 길이가 서로 다른 경우 Melspectrogram, MFCC 모두 길이가 다르므로, 고정 사이즈를 정하고 랜덤으로 앞과 뒤에 Padding
- reqlen: 필요한 최종 데이터 길이
  - 입력 데이터의 길이가 reqlen보다 길 경우, reqlen에 맞게 데이터를 잘라서 반환
  - 입력 데이터의 길이가 reqlen과 같을 경우, 그대로 반환
  - 입력 데이터의 길이가 reqlen보다 짧을 경우, 부족한 부분을 랜덤한 값으로 채워서 데이터의 길이를 reqlen에 맞게 확장
"""

spectrogram = nn.Sequential(
    AT.Spectrogram(n_fft=512,
                   win_length=400,
                   hop_length=160),
    AT.AmplitudeToDB()
)

spec = spectrogram(sample_data)

"""# 2. Audio Feature Extraction
**Spectrogram**
- Shape: [주파수 방향 성분 수 (n_fft / 2 + 1, 0Hz 부터 sample_rate의 절반), Time 방향 성분 수]
  - sample_rate의 절반인 이유 ㅣ Nyquist Frequency
- torchaudio, nn.Sequential
  - AmplitudeToDB: Power 단위의 Spectrogram 또는 Melspectrogram을 dB(로그) 단위로 변환
    - dB 단위는 딥러닝 모델이 이해하기 편한 범위의 값을 제공
  - n_fft: win_length의 크기로 잘린 음성의 작은 조각은 0으로 Padding 되어서 n_fft로 크기가 맞춰짐
    - 따라서, n_fft는 win_length 보다 크거나 같아야 하고 일반적으로 속도를 위해서 2^n의 값으로 설정
    - Padding된 조각에 Fourier Transform이 적용되는 것
  - win_length: 음성을 잘라서 생기는 작은 조각의 크기
    - 16000Hz인 음성에서는 400에 해당하는 값입니다.
  - hop_length: 음성을 작은 조각으로 자를 때 자르는 간격에 해당
    - 16000Hz인 음성에서는 160에 해당하는 값
"""

# Library 1: Librosa
S = librosa.feature.melspectrogram(data, sr=sample_rate, n_mels=40)
log_S = librosa.power_to_db(S, ref=np.max)
librosa.display.specshow(log_S, sr=sr)  # x axis: Time, y axis: Frequency

"""# Melspectrogram
- 음성데이터를 Frequency, Time의 2차원 도메인으로 변환
- Shape: [주파수 방향 성분 수, Time 방향 성분 수]
- librosa.feature.melspectrogram
  - n_mels: Melspectrogram의 주파수 해상도를 조절
  - power_to_db: Melspectrogram을 데시벨로 스케일링하여 특징을 강조
  - ref=np.max: 데시벨로 변환 시 사용되는 참조 값
"""

# Library 2: Torchaudio
mel_spectrogram = nn.Sequential(
    AT.MelSpectrogram(sample_rate=22,
                      n_fft=512,
                      win_length=400,
                      hop_length=160,
                      n_mels=80),
    AT.AmplitudeToDB()
)
mel = mel_spectrogram(sample_data)

"""- torchaudio, nn.Sequential
    - n_mels: 적용할 Mel Filter의 개수를 의미

- win_length가 커질수록 주파수 성분에 대한 해상도는 높아지지만, 시간 성분에 대한 해상도는 낮아짐
- win_length가 작은 경우에는 주파수 성분에 대한 해상도는 낮아지지만, 시간 성분에 대한 해상도는 높아짐
- n_fft를 높이면 주파수 성분의 수는 증가하지만 실제 주파수의 해상도는 증가하지 않음
"""

# MFCC

mfcc = librosa.feature.mfcc(data, sr=sample_rate, n_mels=40)
librosa.display.specshow(mfcc, sr=sample_rate)  # x axis: Time, y axis: MFCC coefficients

# Stacked Melspectrogram
# Melspectrogram의 1차 미분과 2차 미분을 채널로 Stacking 하여 음성에 대한 변화율 정보를 포함하는 데이터 생성

def stacked_melspectrogram(data):

    frame_length = 0.025
    frame_stride = 0.010

    input_nfft = int(round(sampling_rate*frame_length))
    input_stride = int(round(sampling_rate*frame_stride))

    extracted_features = []
    for i in data:

        if feature == "mfcc":
            n_feature = 40
            S = librosa.feature.mfcc(y=i,
                                     sr=sampling_rate,
                                     n_mfcc=n_feature,
                                     n_fft=input_nfft,
                                     hop_length=input_stride)
            S_delta = librosa.feature.delta(S)
            S_delta2 = librosa.feature.delta(S, order=2)

        elif feature == "melspec":
            n_feature = 128
            S = librosa.feature.melspectrogram(y=i,
                                               sr=sampling_rate,
                                               n_mels=n_feature,
                                               n_fft=input_nfft,
                                               hop_length=input_stride)
            S = librosa.power_to_db(S, ref=np.max)
            S_delta = librosa.feature.delta(S)
            S_delta2 = librosa.feature.delta(S, order=2)

        S = np.stack((S, S_delta, S_delta2), axis=2)
        extracted_features.append(S)

    return np.array(extracted_features)

# Multi-Resolution Melspctrogram
# 4가지의 서로 다른 win_length로 다양한 해상도를 가진 Melspectrogram들을 Stacking하고 Normalizing

def melspectrogram(win_length):
        mels = nn.Sequential(
            AT.MelSpectrogram(sample_rate=16000, n_fft=2048, win_length=win_length,
                              hop_length=100, pad=50,f_min=25,f_max=7500,n_mels=160),
            AT.AmplitudeToDB())
        return mels

mels_1 = melspectrogram(250)[0, :, 1:-1]
mels_2 = melspectrogram(500)[0, :, 1:-1]
mels_3 = melspectrogram(750)[0, :, 1:-1]
mels_4 = melspectrogram(1000)[0, :, 1:-1]

device = 'cuda' if torch.cuda.is_available() else 'cpu'
cali = torch.linspace(-0.5, 0.5, steps=160, device=device).view(1, -1, 1)

stacked_mels = torch.stack([mels_1, mels_2, mels_3, mels_4], dim=0)
stacked_mels = (stacked_mels - stacked_mels.mean(dim=[1, 2], keepdim=True)) / 20 + cali

# Noising
# 랜덤 노이즈 추가

def noising(data,noise_factor):
    noise = np.random.randn(len(data))
    augmented_data = data + noise_factor * noise
    augmented_data = augmented_data.astype(type(data[0]))
    return augmented_data

# Shifting
# 좌우로 이동

def shifting(data, sampling_rate, shift_max, shift_direction):
    shift = np.random.randint(sampling_rate * shift_max+1)
    if shift_direction == 'right':
        shift = -shift
    elif shift_direction == 'both':
        direction = np.random.randint(0, 2)
        if direction == 1:
            shift = -shift
    augmented_data = np.roll(data, shift)
    # Set to silence for heading/ tailing
    if shift > 0:
        augmented_data[:shift] = 0
    else:
        augmented_data[shift:] = 0
    return augmented_data

# Pitch
# 피치 조절

def change_pitch(data, sampling_rate, pitch_factor):
    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)

# SpecAugment

time_mask = 32
freq_mask = 32
n_time_mask = 1
n_freq_mask = 1

def SpecAugment(spec, T=time_mask, F=freq_mask, time_mask_num=n_time_mask, freq_mask_num=n_freq_mask):
    feat_size = spec.shape[0]
    seq_len = spec.shape[1]

    # freq mask
    for _ in range(freq_mask_num):
        f = np.random.uniform(low=0.0, high=F)
        f = int(f)
        f0 = random.randint(0, feat_size - f)
        spec[:, f0 : f0 + f] = 0
    # time mask
    for _ in range(time_mask_num):
        t = np.random.uniform(low=0.0, high=T)
        t = int(t)
        t0 = random.randint(0, seq_len - t)
        spec[t0 : t0 + t] = 0
    return spec

"""- feat_size 및 seq_len을 계산하여 Spectrogram의 크기를 가져옴
- freq_mask를 적용하여 주어진 freq_mask_num만큼 주파수 축에서 일부 주파수를 무작위로 제거하여 데이터를 변형
- time_mask를 적용하여 주어진 time_mask_num만큼 시간 축에서 일부 시간적인 정보를 무작위로 제거하여 데이터를 변형
"""