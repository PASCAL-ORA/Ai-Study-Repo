#!/usr/bin/env python
# coding: utf-8

# In[1]:


import os
import json
import re
import torch
import librosa
import pandas as pd
from tqdm import tqdm
from jiwer import cer
from random import sample
from transformers import WhisperProcessor, WhisperForConditionalGeneration

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (GPUìš©)
get_ipython().system('pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121')
get_ipython().system('pip install librosa jiwer transformers tqdm pandas')

# ê²½ë¡œ ì„¤ì •
AUDIO_DIR = r"D:\AIhub_data\01.ì›ì²œë°ì´í„°"
LABEL_DIR = r"D:\AIhub_data\02.ë¼ë²¨ë§ë°ì´í„°"
OUTPUT_CSV = "cer_results.csv"

# ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "openai/whisper-small"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)


# In[2]:


# í•œê¸€ ì •ê·œí™” í•¨ìˆ˜
def normalize_korean(text):
    text = re.sub(r"[^ã„±-ã…ê°€-í£0-9a-zA-Z\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text.lower()

# ì˜¤ë””ì˜¤ ë¶ˆëŸ¬ì˜¤ê¸°
def load_audio(file_path, target_sr=16000):
    audio, _ = librosa.load(file_path, sr=target_sr)
    return audio


# In[3]:


# Whisper ìŒì„± ì¸ì‹
def transcribe(audio):
    inputs = processor(audio, sampling_rate=16000, return_tensors="pt", return_attention_mask=True)
    forced_decoder_ids = processor.get_decoder_prompt_ids(language="ko", task="transcribe")
    outputs = model.generate(
        inputs.input_features.to(device),
        attention_mask=inputs.attention_mask.to(device),
        forced_decoder_ids=forced_decoder_ids
    )
    return processor.batch_decode(outputs, skip_special_tokens=True)[0]

# ì˜¤ë””ì˜¤ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (4000ê°œ ìƒ˜í”Œë§)
all_audio_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith(".wav")]
audio_files = sample(all_audio_files, min(4000, len(all_audio_files)))


# In[4]:


import torch
print("GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€:", torch.cuda.is_available())
print("ì‚¬ìš© ê°€ëŠ¥í•œ GPU ê°œìˆ˜:", torch.cuda.device_count())
print("í˜„ì¬ GPU ì´ë¦„:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "ì—†ìŒ")


# In[5]:


# í‰ê°€ ì‹œì‘
results = []
total_cer = 0
count = 0

print("ğŸ’¬ CER í‰ê°€ ì‹œì‘...")

for audio_file in tqdm(audio_files, desc=" Evaluating", unit="file"):
    file_id = os.path.splitext(audio_file)[0]
    audio_path = os.path.join(AUDIO_DIR, audio_file)
    json_path = os.path.join(LABEL_DIR, file_id + ".json")

    if not os.path.exists(json_path):
        print(f"[WARN] ë¼ë²¨ë§ ëˆ„ë½: {file_id}")
        continue

    try:
        # ì •ë‹µ ë¬¸ì¥ ë¶ˆëŸ¬ì˜¤ê¸°
        with open(json_path, "r", encoding="utf-8") as jf:
            data = json.load(jf)
            reference = data.get("transcription", {}).get("standard", "").strip()

        if not reference:
            print(f"[WARN] ì •ë‹µ ëˆ„ë½: {file_id}")
            continue

        # ìŒì„± â†’ í…ìŠ¤íŠ¸
        audio = load_audio(audio_path)
        hypothesis = transcribe(audio)

        # ì •ê·œí™” ë° CER ê³„ì‚°
        ref_norm = normalize_korean(reference)
        hyp_norm = normalize_korean(hypothesis)
        file_cer = cer(ref_norm, hyp_norm)

        # ëˆ„ì 
        total_cer += file_cer
        count += 1

        results.append({
            "file_id": file_id,
            "audio_file": audio_file,
            "reference": ref_norm,
            "hypothesis": hyp_norm,
            "cer": round(file_cer, 3)
        })

    except Exception as e:
        print(f"[ERROR] {file_id}: {e}")

# ê²°ê³¼ ì €ì¥
df = pd.DataFrame(results)
df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")

# í‰ê·  CER ì¶œë ¥
if count > 0:
    print(f"\nâœ… í‰ê·  CER: {total_cer / count:.3f} ({count}ê°œ íŒŒì¼ ê¸°ì¤€)")
else:
    print("âŒ í‰ê°€í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")


# In[1]:


import os
print(os.path.abspath("cer_results.csv"))

